{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel,LdaMulticore\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "fp = codecs.open('data/LDA_train_data.txt','r',encoding='utf8')\n",
    "for line in fp:\n",
    "    line = line.split()\n",
    "    train.append([ w for w in line  ])\n",
    "dictionary = corpora.Dictionary(train)\n",
    "corpus = [ dictionary.doc2bow(text) for text in train ]\n",
    "lda = LdaMulticore(workers=3,corpus=corpus, id2word=dictionary, num_topics=50,passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.019*\"美丽\" + 0.012*\"周末\" + 0.011*\"家乡\" + 0.008*\"雪景\" + 0.007*\"季节\" + 0.007*\"一场\" + 0.006*\"秋天\" + 0.006*\"春天\" + 0.005*\"⋯\" + 0.005*\"冬天\"')\n",
      "(1, '0.047*\"走\" + 0.037*\"下雪\" + 0.028*\"有人\" + 0.024*\"努力\" + 0.022*\"路\" + 0.017*\"成功\" + 0.016*\"做\" + 0.014*\"地方\" + 0.011*\"越\" + 0.009*\"梦想\"')\n",
      "(2, '0.063*\"em\" + 0.026*\"生日快乐\" + 0.018*\"亲爱\" + 0.015*\"一半\" + 0.011*\"祝\" + 0.007*\"生日\" + 0.005*\"村长\" + 0.005*\"考\" + 0.005*\"接受\" + 0.004*\"善待\"')\n",
      "(3, '0.011*\"女生\" + 0.010*\"男生\" + 0.007*\"蛋糕\" + 0.007*\"à\" + 0.007*\"杀\" + 0.006*\"一家\" + 0.005*\"安逸\" + 0.005*\"知己\" + 0.004*\"读\" + 0.004*\"友谊\"')\n",
      "(4, '0.125*\"女人\" + 0.101*\"男人\" + 0.036*\"喜欢\" + 0.009*\"爱\" + 0.007*\"漂亮\" + 0.006*\"穿\" + 0.005*\"做\" + 0.005*\"女性\" + 0.005*\"女\" + 0.005*\"身材\"')\n",
      "(5, '0.033*\"笑\" + 0.012*\"俄\" + 0.007*\"爹\" + 0.006*\"娘\" + 0.005*\"自已\" + 0.005*\"感激\" + 0.005*\"永不\" + 0.005*\"泡\" + 0.005*\"太快\" + 0.005*\"那种\"')\n",
      "(6, '0.067*\"朋友\" + 0.056*\"快乐\" + 0.044*\"历友们\" + 0.032*\"谢谢\" + 0.023*\"祝\" + 0.019*\"请\" + 0.019*\"好看\" + 0.019*\"健康\" + 0.015*\"愿\" + 0.014*\"祝福\"')\n",
      "(7, '0.079*\"想\" + 0.059*\"冷\" + 0.047*\"无聊\" + 0.032*\"找\" + 0.025*\"工作\" + 0.017*\"难\" + 0.017*\"日子\" + 0.016*\"好烦\" + 0.013*\"有没有\" + 0.013*\"找个\"')\n",
      "(8, '0.036*\"元\" + 0.034*\"美\" + 0.022*\"牙\" + 0.019*\"呲\" + 0.007*\"100\" + 0.007*\"嗨\" + 0.006*\"免费\" + 0.005*\"天\" + 0.004*\"y\" + 0.004*\"....\"')\n",
      "(9, '0.059*\"太阳\" + 0.040*\"终于\" + 0.016*\"睡觉\" + 0.010*\"老板\" + 0.010*\"见到\" + 0.010*\"疼\" + 0.009*\"暖\" + 0.009*\"太冷\" + 0.008*\"礼物\" + 0.007*\"好久\"')\n",
      "(10, '0.054*\"年\" + 0.043*\"月\" + 0.029*\"3\" + 0.029*\"岁\" + 0.023*\"1\" + 0.023*\"2\" + 0.021*\"5\" + 0.019*\"4\" + 0.018*\"日\" + 0.018*\"10\"')\n",
      "(11, '0.031*\"漂亮\" + 0.017*\"早上好\" + 0.016*\"●\" + 0.013*\"代表\" + 0.011*\"减肥\" + 0.009*\"美食\" + 0.009*\"胖\" + 0.008*\"IS\" + 0.008*\"超\" + 0.008*\"博物馆\"')\n",
      "(12, '0.029*\"买\" + 0.027*\"手机\" + 0.011*\"公司\" + 0.010*\"车\" + 0.008*\"积分\" + 0.008*\"￼\" + 0.007*\"卖\" + 0.007*\"银行\" + 0.006*\"市场\" + 0.006*\"价格\"')\n",
      "(13, '0.083*\"宝宝\" + 0.031*\"女\" + 0.019*\"男\" + 0.009*\"雪花\" + 0.008*\"网友\" + 0.008*\"等待\" + 0.007*\"洗澡\" + 0.007*\"皮肤\" + 0.005*\"过生日\" + 0.005*\"怎么回事\"')\n",
      "(14, '0.016*\"力量\" + 0.008*\"小雪\" + 0.008*\"这天\" + 0.006*\"赢\" + 0.006*\"画\" + 0.006*\"鞋子\" + 0.005*\"知识\" + 0.005*\"暖气\" + 0.005*\"贵人\" + 0.004*\"树\"')\n",
      "(15, '0.008*\"人民币\" + 0.005*\"阳光明媚\" + 0.005*\"女神\" + 0.005*\"阿里\" + 0.005*\"亚洲\" + 0.005*\"猫\" + 0.004*\"君\" + 0.004*\"米\" + 0.004*\"星期天\" + 0.004*\"cp\"')\n",
      "(16, '0.031*\"晒晒\" + 0.017*\"烦死\" + 0.013*\"换\" + 0.011*\"女友\" + 0.009*\"可怕\" + 0.009*\"人渣\" + 0.007*\"✌\" + 0.007*\"相片\" + 0.006*\"冷冷\" + 0.006*\"办\"')\n",
      "(17, '0.026*\"单身\" + 0.020*\"结束\" + 0.017*\"快点\" + 0.014*\"看不到\" + 0.014*\"适合\" + 0.010*\"十年\" + 0.010*\"姑娘\" + 0.010*\"可惜\" + 0.007*\"聊聊\" + 0.007*\"喔\"')\n",
      "(18, '0.010*\"做\" + 0.010*\"中\" + 0.008*\"说\" + 0.008*\"时\" + 0.007*\"事情\" + 0.006*\"生活\" + 0.006*\"婚姻\" + 0.005*\"改变\" + 0.005*\"一种\" + 0.004*\"家庭\"')\n",
      "(19, '0.017*\"没什么\" + 0.014*\"没人\" + 0.008*\"闺蜜\" + 0.006*\"寒冷\" + 0.006*\"平平淡淡\" + 0.005*\"Hailey\" + 0.005*\"低头\" + 0.004*\"南无\" + 0.004*\"小女孩\" + 0.004*\"厚\"')\n",
      "(20, '0.050*\"心情\" + 0.035*\"真的\" + 0.032*\"感觉\" + 0.031*\"不好\" + 0.030*\"开心\" + 0.028*\"明天\" + 0.027*\"上班\" + 0.023*\"真\" + 0.022*\"下雨\" + 0.021*\"玩\"')\n",
      "(21, '0.024*\"1\" + 0.017*\"2\" + 0.012*\"3\" + 0.011*\"克\" + 0.010*\"分钟\" + 0.008*\"4\" + 0.008*\"5\" + 0.008*\"水\" + 0.008*\"放入\" + 0.007*\"10\"')\n",
      "(22, '0.043*\"累\" + 0.036*\"不想\" + 0.022*\"起床\" + 0.022*\"玫瑰\" + 0.010*\"警察\" + 0.010*\"不行\" + 0.010*\"土耳其\" + 0.010*\"l\" + 0.008*\"骗子\" + 0.007*\"晴\"')\n",
      "(23, '0.027*\"感恩\" + 0.021*\"感恩节\" + 0.020*\"晚安\" + 0.013*\"万年历\" + 0.012*\"爱心\" + 0.009*\"中秋节\" + 0.008*\"月\" + 0.008*\"亲们\" + 0.007*\"中华\" + 0.007*\"美景\"')\n",
      "(24, '0.009*\"级\" + 0.008*\"美国\" + 0.008*\"曝光\" + 0.005*\"中国\" + 0.005*\"t\" + 0.005*\"I\" + 0.005*\"to\" + 0.004*\"转\" + 0.004*\"累死\" + 0.004*\"the\"')\n",
      "(25, '0.044*\"钱\" + 0.032*\"这是\" + 0.028*\"花\" + 0.018*\"东西\" + 0.015*\"不知\" + 0.010*\"有钱\" + 0.009*\"答案\" + 0.008*\"难过\" + 0.008*\"说\" + 0.007*\"买\"')\n",
      "(26, '0.016*\"女孩\" + 0.013*\"说\" + 0.011*\"跑\" + 0.010*\"名字\" + 0.009*\"没事\" + 0.009*\"开车\" + 0.008*\"男孩\" + 0.008*\"男子\" + 0.007*\"告诉\" + 0.007*\"医院\"')\n",
      "(27, '0.051*\"加油\" + 0.029*\"新\" + 0.013*\"❤\" + 0.010*\"称\" + 0.010*\"遭\" + 0.008*\"员工\" + 0.008*\"广州\" + 0.008*\"大爷\" + 0.007*\"老人\" + 0.007*\"当兵\"')\n",
      "(28, '0.023*\"•\" + 0.015*\"鱼\" + 0.012*\"☀\" + 0.011*\"运动\" + 0.009*\"๑\" + 0.007*\"黑\" + 0.007*\"大宝\" + 0.006*\"锻炼\" + 0.005*\"法王\" + 0.005*\"噶玛巴\"')\n",
      "(29, '0.052*\"中国\" + 0.019*\"美国\" + 0.017*\"日本\" + 0.011*\"国家\" + 0.007*\"世界\" + 0.006*\"约\" + 0.005*\"历史\" + 0.005*\"时间\" + 0.005*\"胜利\" + 0.004*\"分\"')\n",
      "(30, '0.121*\"孩子\" + 0.045*\"妈妈\" + 0.033*\"父母\" + 0.033*\"儿子\" + 0.023*\"女儿\" + 0.020*\"爸爸\" + 0.017*\"母亲\" + 0.017*\"老师\" + 0.013*\"父亲\" + 0.013*\"冬天\"')\n",
      "(31, '0.035*\"雪\" + 0.026*\"心\" + 0.022*\"世界\" + 0.013*\"众生\" + 0.008*\"中\" + 0.008*\"心灵\" + 0.008*\"解脱\" + 0.007*\"菩萨\" + 0.007*\"佛\" + 0.007*\"自我\"')\n",
      "(32, '0.056*\"越来越\" + 0.014*\"好大\" + 0.013*\"无奈\" + 0.013*\"赞\" + 0.012*\"☺\" + 0.010*\"老天\" + 0.007*\"好想你\" + 0.007*\"羊\" + 0.006*\"厉友们\" + 0.006*\"好难\"')\n",
      "(33, '0.074*\"吃\" + 0.018*\"喝\" + 0.012*\"食物\" + 0.011*\"滑\" + 0.007*\"中\" + 0.007*\"健康\" + 0.006*\"水果\" + 0.005*\"体内\" + 0.005*\"少\" + 0.005*\"饮食\"')\n",
      "(34, '0.026*\"爱\" + 0.024*\"人生\" + 0.016*\"幸福\" + 0.012*\"生活\" + 0.010*\"心\" + 0.010*\"一种\" + 0.009*\"珍惜\" + 0.009*\"世界\" + 0.008*\"永远\" + 0.008*\"生命\"')\n",
      "(35, '0.062*\"说\" + 0.022*\"老公\" + 0.020*\"想\" + 0.012*\"做\" + 0.011*\"结婚\" + 0.011*\"钱\" + 0.010*\"老婆\" + 0.010*\"真的\" + 0.008*\"孩子\" + 0.008*\"离婚\"')\n",
      "(36, '0.022*\"晚上\" + 0.022*\"睡不着\" + 0.022*\"咯\" + 0.020*\"拍\" + 0.016*\"失眠\" + 0.011*\"度\" + 0.010*\"降温\" + 0.009*\"走走\" + 0.009*\"空气\" + 0.008*\"温度\"')\n",
      "(37, '0.048*\"天气\" + 0.024*\"可爱\" + 0.016*\"文物\" + 0.015*\"热\" + 0.012*\"今日\" + 0.011*\"美美\" + 0.007*\"阴天\" + 0.006*\"滴\" + 0.006*\"主席\" + 0.006*\"哒\"')\n",
      "(38, '0.024*\"梦\" + 0.017*\"早安\" + 0.011*\"★\" + 0.008*\"多久\" + 0.007*\"景色\" + 0.007*\"心累\" + 0.006*\"夜班\" + 0.005*\"宽恕\" + 0.005*\"真理\" + 0.005*\"☔\"')\n",
      "(39, '0.172*\"生活\" + 0.080*\"希望\" + 0.070*\"微笑\" + 0.022*\"烦恼\" + 0.016*\"想要\" + 0.016*\"我要\" + 0.011*\"简单\" + 0.011*\"压力\" + 0.009*\"工作\" + 0.009*\"幸福\"')\n",
      "(40, '0.013*\"号\" + 0.013*\"说\" + 0.012*\"回家\" + 0.012*\"字\" + 0.011*\"杞县\" + 0.010*\"法院\" + 0.009*\"孩子\" + 0.009*\"年\" + 0.008*\"一年\" + 0.006*\"抓\"')\n",
      "(41, '0.016*\"好吃\" + 0.013*\"看着\" + 0.011*\"饿\" + 0.008*\"行\" + 0.007*\"回到\" + 0.006*\"里\" + 0.006*\"声音\" + 0.005*\"一块\" + 0.005*\"情侣\" + 0.005*\"强\"')\n",
      "(42, '0.021*\"历友\" + 0.016*\"️\" + 0.013*\"西安\" + 0.012*\"雾\" + 0.012*\"感谢\" + 0.012*\"霾\" + 0.009*\"大唐\" + 0.008*\"晴天\" + 0.007*\"今天天气\" + 0.007*\"ó\"')\n",
      "(43, '0.022*\"说\" + 0.018*\"吃饭\" + 0.012*\"麻烦\" + 0.012*\"狼\" + 0.011*\"深圳\" + 0.010*\"老兵\" + 0.010*\"听说\" + 0.009*\"兄弟\" + 0.009*\"俗话说\" + 0.008*\"俗话\"')\n",
      "(44, '0.013*\"身体\" + 0.011*\"时\" + 0.007*\"医生\" + 0.006*\"中\" + 0.006*\"时间\" + 0.005*\"睡\" + 0.005*\"健康\" + 0.005*\"导致\" + 0.005*\"情况\" + 0.005*\"影响\"')\n",
      "(45, '0.023*\"偷笑\" + 0.023*\"生日\" + 0.015*\"忙\" + 0.014*\"记得\" + 0.011*\"话费\" + 0.010*\"脸上\" + 0.010*\"正\" + 0.010*\"评论\" + 0.009*\"能量\" + 0.008*\"分享\"')\n",
      "(46, '0.036*\"宝贝\" + 0.024*\"美女\" + 0.022*\"\\ue032\" + 0.019*\"我家\" + 0.011*\"懂\" + 0.009*\"公园\" + 0.009*\"o\" + 0.009*\"⊙\" + 0.008*\"旅行\" + 0.008*\"幸福\"')\n",
      "(47, '0.024*\"难受\" + 0.023*\"感冒\" + 0.009*\"领导\" + 0.008*\"想念\" + 0.006*\"女子\" + 0.006*\"头疼\" + 0.005*\"厉害\" + 0.005*\"迟到\" + 0.004*\"孙悟空\" + 0.004*\"咳嗽\"')\n",
      "(48, '0.041*\"晒\" + 0.023*\"照片\" + 0.017*\"狗\" + 0.016*\"电影\" + 0.015*\"哒\" + 0.014*\"属\" + 0.009*\"牛\" + 0.008*\"晒太阳\" + 0.008*\"猪\" + 0.007*\"喷\"')\n",
      "(49, '0.008*\"高\" + 0.008*\"中\" + 0.007*\"维生素\" + 0.007*\"感冒\" + 0.007*\"效果\" + 0.006*\"钙\" + 0.006*\"曝\" + 0.006*\"头发\" + 0.005*\"原则\" + 0.004*\"阿胶\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in lda.print_topics(num_topics=50,num_words = 10):\n",
    "#     termNumber = topic[0]\n",
    "#     print(topic[0], ':', sep='')\n",
    "#     listOfTerms = topic[1].split('+')\n",
    "#     for term in listOfTerms:\n",
    "#         listItems = term.split('*')\n",
    "#         print('  ', listItems[1], '(', listItems[0], ')', sep='')\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LdaModel in module gensim.models.ldamodel:\n",
      "\n",
      "class LdaModel(gensim.interfaces.TransformationABC, gensim.models.basemodel.BaseTopicModel)\n",
      " |  The constructor estimates Latent Dirichlet Allocation model parameters based\n",
      " |  on a training corpus:\n",
      " |  \n",
      " |  >>> lda = LdaModel(corpus, num_topics=10)\n",
      " |  \n",
      " |  You can then infer topic distributions on new, unseen documents, with\n",
      " |  \n",
      " |  >>> doc_lda = lda[doc_bow]\n",
      " |  \n",
      " |  The model can be updated (trained) with new documents via\n",
      " |  \n",
      " |  >>> lda.update(other_corpus)\n",
      " |  \n",
      " |  Model persistency is achieved through its `load`/`save` methods.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      gensim.models.basemodel.BaseTopicModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=None)\n",
      " |      Args:\n",
      " |          bow (list): Bag-of-words representation of a document.\n",
      " |          eps (float): Ignore topics with probability below `eps`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topic distribution for the given document `bow`, as a list of\n",
      " |          `(topic_id, topic_probability)` 2-tuples.\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None)\n",
      " |      If given, start training from the iterable `corpus` straight away. If not given,\n",
      " |      the model is left untrained (presumably because you want to call `update()` manually).\n",
      " |      \n",
      " |      `num_topics` is the number of requested latent topics to be extracted from\n",
      " |      the training corpus.\n",
      " |      \n",
      " |      `id2word` is a mapping from word ids (integers) to words (strings). It is\n",
      " |      used to determine the vocabulary size, as well as for debugging and topic\n",
      " |      printing.\n",
      " |      \n",
      " |      `alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
      " |      (theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
      " |      1.0/num_topics prior.\n",
      " |      \n",
      " |      `alpha` can be set to an explicit array = prior of your choice. It also\n",
      " |      support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
      " |      normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
      " |      prior directly from your data.\n",
      " |      \n",
      " |      `eta` can be a scalar for a symmetric prior over topic/word\n",
      " |      distributions, or a vector of shape num_words, which can be used to\n",
      " |      impose (user defined) asymmetric priors over the word distribution.\n",
      " |      It also supports the special value 'auto', which learns an asymmetric\n",
      " |      prior over words directly from your data. `eta` can also be a matrix\n",
      " |      of shape num_topics x num_words, which can be used to impose\n",
      " |      asymmetric priors over the word distribution on a per-topic basis\n",
      " |      (can not be learned from data).\n",
      " |      \n",
      " |      Turn on `distributed` to force distributed computing (see the `web tutorial <http://radimrehurek.com/gensim/distributed.html>`_\n",
      " |      on how to set up a cluster of machines for gensim).\n",
      " |      \n",
      " |      Calculate and log perplexity estimate from the latest mini-batch every\n",
      " |      `eval_every` model updates (setting this to 1 slows down training ~2x;\n",
      " |      default is 10 for better performance). Set to None to disable perplexity estimation.\n",
      " |      \n",
      " |      `decay` and `offset` parameters are the same as Kappa and Tau_0 in\n",
      " |      Hoffman et al, respectively.\n",
      " |      \n",
      " |      `minimum_probability` controls filtering the topics returned for a document (bow).\n",
      " |      \n",
      " |      `random_state` can be a np.random.RandomState object or the seed for one\n",
      " |      \n",
      " |      `callbacks` a list of metric callbacks to log/visualize evaluation metrics of topic model during training\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> lda = LdaModel(corpus, num_topics=100)  # train model\n",
      " |      >>> print(lda[doc_bow]) # get topic probability distribution for a document\n",
      " |      >>> lda.update(corpus2) # update the LDA model with additional documents\n",
      " |      >>> print(lda[doc_bow])\n",
      " |      \n",
      " |      >>> lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from `corpus`:\n",
      " |      E_q[log p(corpus)] - E_q[log q(corpus)]\n",
      " |      \n",
      " |      Args:\n",
      " |          corpus: documents to infer variational bounds from.\n",
      " |          gamma: the variational parameters on topic weights for each `corpus`\n",
      " |              document (=2d matrix=what comes out of `inference()`).\n",
      " |              If not supplied, will be inferred from the model.\n",
      " |          subsample_ratio (float): If `corpus` is a sample of the whole corpus,\n",
      " |              pass this to inform on what proportion of the corpus it represents.\n",
      " |              This is used as a multiplicative factor to scale the likelihood\n",
      " |              appropriately.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The variational bound score calculated.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear model state (free up some memory). Used in the distributed algo.\n",
      " |  \n",
      " |  diff(self, other, distance='kullback_leibler', num_words=100, n_ann_terms=10, diagonal=False, annotation=True, normed=True)\n",
      " |      Calculate difference topic2topic between two Lda models\n",
      " |      `other` instances of `LdaMulticore` or `LdaModel`\n",
      " |      `distance` is function that will be applied to calculate difference between any topic pair.\n",
      " |      Available values: `kullback_leibler`, `hellinger`, `jaccard` and `jensen_shannon`\n",
      " |      `num_words` is quantity of most relevant words that used if distance == `jaccard` (also used for annotation)\n",
      " |      `n_ann_terms` is max quantity of words in intersection/symmetric difference between topics (used for annotation)\n",
      " |      `diagonal` set to True if the difference is required only between the identical topic no.s (returns diagonal of diff matrix)\n",
      " |      `annotation` whether the intersection or difference of words between two topics should be returned\n",
      " |      Returns a matrix Z with shape (m1.num_topics, m2.num_topics), where Z[i][j] - difference between topic_i and topic_j\n",
      " |      and matrix annotation (if True) with shape (m1.num_topics, m2.num_topics, 2, None),\n",
      " |      where:\n",
      " |      \n",
      " |          annotation[i][j] = [[`int_1`, `int_2`, ...], [`diff_1`, `diff_2`, ...]] and\n",
      " |          `int_k` is word from intersection of `topic_i` and `topic_j` and\n",
      " |          `diff_l` is word from symmetric difference of `topic_i` and `topic_j`\n",
      " |          `normed` is a flag. If `true`, matrix Z will be normalized\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> m1, m2 = LdaMulticore.load(path_1), LdaMulticore.load(path_2)\n",
      " |      >>> mdiff, annotation = m1.diff(m2)\n",
      " |      >>> print(mdiff) # get matrix with difference for each topic pair from `m1` and `m2`\n",
      " |      >>> print(annotation) # get array with positive/negative words for each topic pair from `m1` and `m2`\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected\n",
      " |      sufficient statistics in `state` (or `self.state` if None).\n",
      " |  \n",
      " |  do_mstep(self, rho, other, extra_pass=False)\n",
      " |      M step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |  \n",
      " |  get_document_topics(self, bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
      " |      Args:\n",
      " |          bow (list): Bag-of-words representation of the document to get topics for.\n",
      " |          minimum_probability (float): Ignore topics with probability below this value\n",
      " |              (None by default). If set to None, a value of 1e-8 is used to prevent 0s.\n",
      " |          per_word_topics (bool): If True, also returns a list of topics, sorted in\n",
      " |              descending order of most likely topics for that word. It also returns a list\n",
      " |              of word_ids and each words corresponding topics' phi_values, multiplied by\n",
      " |              feature length (i.e, word count).\n",
      " |          minimum_phi_value (float): if `per_word_topics` is True, this represents a lower\n",
      " |              bound on the term probabilities that are included (None by default). If set\n",
      " |              to None, a value of 1e-8 is used to prevent 0s.\n",
      " |      \n",
      " |      Returns:\n",
      " |          topic distribution for the given document `bow`, as a list of\n",
      " |          `(topic_id, topic_probability)` 2-tuples.\n",
      " |  \n",
      " |  get_term_topics(self, word_id, minimum_probability=None)\n",
      " |      Args:\n",
      " |          word_id (int): ID of the word to get topic probabilities for.\n",
      " |          minimum_probability (float): Only include topic probabilities above this\n",
      " |              value (None by default). If set to None, use 1e-8 to prevent including 0s.\n",
      " |      Returns:\n",
      " |          list: The most likely topics for the given word. Each topic is represented\n",
      " |          as a tuple of `(topic_id, term_probability)`.\n",
      " |  \n",
      " |  get_topic_terms(self, topicid, topn=10)\n",
      " |      Args:\n",
      " |          topn (int): Only return 2-tuples for the topn most probable words\n",
      " |              (ignore the rest).\n",
      " |      \n",
      " |      Returns:\n",
      " |          list: `(word_id, probability)` 2-tuples for the most probable words\n",
      " |          in topic with id `topicid`.\n",
      " |  \n",
      " |  get_topics(self)\n",
      " |      Returns:\n",
      " |          np.ndarray: `num_topics` x `vocabulary_size` array of floats which represents\n",
      " |          the term topic matrix learned during inference.\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters\n",
      " |      controlling the topic weights) for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model (=is read-only aka const). The\n",
      " |      whole input chunk of document is assumed to fit in RAM; chunking of a\n",
      " |      large corpus must be done earlier in the pipeline.\n",
      " |      \n",
      " |      If `collect_sstats` is True, also collect sufficient statistics needed\n",
      " |      to update the model's topic-word distributions, and return a 2-tuple\n",
      " |      `(gamma, sstats)`. Otherwise, return `(gamma, None)`. `gamma` is of shape\n",
      " |      `len(chunk) x self.num_topics`.\n",
      " |      \n",
      " |      Avoids computing the `phi` variational parameter directly using the\n",
      " |      optimization presented in **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\n",
      " |  \n",
      " |  init_dir_prior(self, prior, name)\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using the `chunk` of\n",
      " |      documents as evaluation corpus. Also output the calculated statistics. incl.\n",
      " |      perplexity=2^(-bound), to log at INFO level.\n",
      " |  \n",
      " |  save(self, fname, ignore=('state', 'dispatcher'), separately=None, *args, **kwargs)\n",
      " |      Save the model to file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |      \n",
      " |      `separately` can be used to define which arrays should be stored in separate files.\n",
      " |      \n",
      " |      `ignore` parameter can be used to define which variables should be ignored, i.e. left\n",
      " |      out from the pickled lda model. By default the internal `state` is ignored as it uses\n",
      " |      its own serialisation not the one provided by `LdaModel`. The `state` and `dispatcher`\n",
      " |      will be added to any ignore parameter defined.\n",
      " |      \n",
      " |      \n",
      " |      Note: do not save as a compressed file if you intend to load the file back with `mmap`.\n",
      " |      \n",
      " |      Note: If you intend to use models across Python 2/3 versions there are a few things to\n",
      " |      keep in mind:\n",
      " |      \n",
      " |        1. The pickled Python dictionaries will not work across Python versions\n",
      " |        2. The `save` method does not automatically save all np arrays using np, only\n",
      " |           those ones that exceed `sep_limit` set in `gensim.utils.SaveLoad.save`. The main\n",
      " |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      " |      \n",
      " |      Please refer to the wiki recipes section (https://github.com/piskvorky/gensim/wiki/Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2)\n",
      " |      for an example on how to work around these issues.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Args:\n",
      " |          topn (int): Only return 2-tuples for the topn most probable words\n",
      " |              (ignore the rest).\n",
      " |      \n",
      " |      Returns:\n",
      " |          list: of `(word, probability)` 2-tuples for the most probable\n",
      " |          words in topic `topicid`.\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      Args:\n",
      " |          num_topics (int): show results for first `num_topics` topics.\n",
      " |              Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |              The returned `num_topics <= self.num_topics` subset of all topics is\n",
      " |              therefore arbitrary and may change between two LDA training runs.\n",
      " |          num_words (int): include top `num_words` with highest probabilities in topic.\n",
      " |          log (bool): If True, log output in addition to returning it.\n",
      " |          formatted (bool): If True, format topics as strings, otherwise return them as\n",
      " |              `(word, probability)` 2-tuples.\n",
      " |      Returns:\n",
      " |          list: `num_words` most significant words for `num_topics` number of topics\n",
      " |          (10 words for top 10 topics, by default).\n",
      " |  \n",
      " |  sync_state(self)\n",
      " |  \n",
      " |  top_topics(self, corpus=None, texts=None, dictionary=None, window_size=None, coherence='u_mass', topn=20, processes=-1)\n",
      " |      Calculate the coherence for each topic; default is Umass coherence.\n",
      " |      \n",
      " |      See the :class:`gensim.models.CoherenceModel` constructor for more info on the\n",
      " |      parameters and the different coherence metrics.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list: tuples with `(topic_repr, coherence_score)`, where `topic_repr` is a list\n",
      " |          of representations of the `topn` terms for the topic. The terms are represented\n",
      " |          as tuples of `(membership_in_topic, token)`. The `coherence_score` is a float.\n",
      " |  \n",
      " |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n",
      " |      Train the model with new documents, by EM-iterating over `corpus` until\n",
      " |      the topics converge (or until the maximum number of allowed iterations\n",
      " |      is reached). `corpus` must be an iterable (repeatable stream of documents),\n",
      " |      \n",
      " |      In distributed mode, the E step is distributed over a cluster of machines.\n",
      " |      \n",
      " |      This update also supports updating an already trained model (`self`)\n",
      " |      with new documents from `corpus`; the two models are then merged in\n",
      " |      proportion to the number of old vs. new documents. This feature is still\n",
      " |      experimental for non-stationary input streams.\n",
      " |      \n",
      " |      For stationary input (no topic drift in new documents), on the other hand,\n",
      " |      this equals the online update of Hoffman et al. and is guaranteed to\n",
      " |      converge for any `decay` in (0.5, 1.0>. Additionally, for smaller\n",
      " |      `corpus` sizes, an increasing `offset` may be beneficial (see\n",
      " |      Table 1 in Hoffman et al.)\n",
      " |      \n",
      " |      Args:\n",
      " |          corpus (gensim corpus): The corpus with which the LDA model should be updated.\n",
      " |      \n",
      " |          chunks_as_numpy (bool): Whether each chunk passed to `.inference` should be a np\n",
      " |              array of not. np can in some settings turn the term IDs\n",
      " |              into floats, these will be converted back into integers in\n",
      " |              inference, which incurs a performance hit. For distributed\n",
      " |              computing it may be desirable to keep the chunks as np\n",
      " |              arrays.\n",
      " |      \n",
      " |      For other parameter settings, see :class:`LdaModel` constructor.\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document\n",
      " |      topic weights `alpha` given the last `gammat`.\n",
      " |  \n",
      " |  update_eta(self, lambdat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-topic\n",
      " |      word weights `eta` given the last `lambdat`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname, *args, **kwargs) from builtins.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      " |      \n",
      " |          >>> LdaModel.load(fname, mmap='r')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n",
      " |  \n",
      " |  print_topic(self, topicno, topn=10)\n",
      " |      Return a single topic as a formatted string. See `show_topic()` for parameters.\n",
      " |      \n",
      " |      >>> lsimodel.print_topic(10, topn=5)\n",
      " |      '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + -0.174 * \"functor\" + -0.168 * \"operator\"'\n",
      " |  \n",
      " |  print_topics(self, num_topics=20, num_words=10)\n",
      " |      Alias for `show_topics()` that prints the `num_words` most\n",
      " |      probable words for `topics` number of topics to log.\n",
      " |      Set `topics=-1` to print all topics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LdaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for topic in lda.get_topics():\n",
    "    count+=1\n",
    "#     termNumber = topic[0]\n",
    "#     print(topic[0], ':', sep='')\n",
    "#     listOfTerms = topic[1].split('+')\n",
    "#     for term in listOfTerms:\n",
    "#         listItems = term.split('*')\n",
    "#         print('  ', listItems[1], '(', listItems[0], ')', sep='')\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lda.get_document_topics(bow=train[0],minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
    "corpus_1 = [ dictionary.doc2bow(train[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = lda.get_document_topics(bow=corpus_1,minimum_probability=None, minimum_phi_value=None, per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
