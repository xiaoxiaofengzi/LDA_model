{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding:utf8 -*-\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def post_cut(url):\n",
    "    fr = open(url + \"/post_data.txt\",encoding='utf8')\n",
    "    fo = open(url + \"/post_key.txt\", \"w\",encoding='utf8')\n",
    "    jieba.analyse.set_stop_words(url + \"/stopwords.txt\")\n",
    "    for line in fr.readlines():\n",
    "        term = line.strip().split(\"\\t\")\n",
    "        if len(term) == 3 and term[2] != \"\":\n",
    "            key_list = jieba.analyse.extract_tags(term[2], 50)  # get keywords\n",
    "#             print(\"key_list is \",key_list)\n",
    "            ustr = term[0] + \"\\t\"\n",
    "            for i in key_list:\n",
    "                # ustr += i.encode(\"utf-8\") + \" \"\n",
    "                ustr += i + \" \"\n",
    "            fo.write(ustr + \"\\n\")\n",
    "    fr.close()\n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_tfidf(url):\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    fr = open(url + \"/post_key.txt\",encoding='utf8')\n",
    "    id_list = []\n",
    "    data_list = []\n",
    "    for line in fr.readlines():\n",
    "        term = line.strip().split(\"\\t\")\n",
    "        if len(term) == 2:\n",
    "            id_list.append(term[0])\n",
    "            data_list.append(term[1])\n",
    "\n",
    "    hv = HashingVectorizer(n_features=10000, non_negative=True)  # 该类实现hash技巧\n",
    "    post_tfidf = hv.fit_transform(data_list)  # return feature vector 'fea_train' [n_samples,n_features]\n",
    "    print('Size of fea_train:' + repr(post_tfidf.shape))\n",
    "    print(post_tfidf.nnz)\n",
    "    post_cluster(url, id_list, post_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_cluster(url, id, tfidf_vec):\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmean = KMeans(n_clusters=300)\n",
    "    print(\"kmeans\")\n",
    "    kmean.fit(tfidf_vec)\n",
    "    print(\"kmeans1\")\n",
    "    #     pred = kmean.transform(tfidf_vec)\n",
    "\n",
    "    #   count1 = 0\n",
    "    #   count2 = 0\n",
    "    #     pred_str = []\n",
    "    #\n",
    "    #     for item in pred:\n",
    "    #         count1 += 1\n",
    "    #         vec = \"\"\n",
    "    #         for tmp in item :\n",
    "    #             vec += str(tmp)[0:7] + \"\\t\"\n",
    "    #         pred_str.append(vec)\n",
    "    #\n",
    "    #     print len(pred_str)\n",
    "    #     print len(id)\n",
    "    count2=0\n",
    "    pred = kmean.predict(tfidf_vec)\n",
    "    fo = open(url + \"/cluster.txt\", \"w\",encoding='utf8')\n",
    "    for i in range(len(pred)):\n",
    "        count2 += 1\n",
    "        fo.write(id[i] + \"\\t\" + str(pred[i]) + \"\\n\")\n",
    "    fo.close()\n",
    "    print(\"kmeans2\")\n",
    "    print(\"%d\" %count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def post_lda(url, cluster):\n",
    "#     from gensim import corpora, models, matutils\n",
    "#     count = 0\n",
    "#     fr = open(url + \"/post_key.txt\",encoding='utf8')\n",
    "#     fo2 = open(url + \"/post_vec_lda.txt\", \"a+\",encoding='utf8')\n",
    "#     id_list = []\n",
    "#     data_list = []\n",
    "\n",
    "#     for line in fr.readlines():\n",
    "#         term = line.strip().split(\"\\t\")\n",
    "#         if len(term) == 2:\n",
    "#             count += 1\n",
    "#             id_list.append(term[0])\n",
    "#             word = term[1].strip().split()\n",
    "#             data_list.append(word)\n",
    "#     print(\"lda\")\n",
    "#     dic = corpora.Dictionary(data_list)  # 构造词典\n",
    "#     corpus = [dic.doc2bow(text) for text in data_list]  # 每个text 对应的稀疏向量\n",
    "# #     print(\"corpus is:\")\n",
    "# #     print(corpus)\n",
    "#     tfidf = models.TfidfModel(corpus)  # 统计tfidf\n",
    "# #     print(\"lda\")\n",
    "#     corpus_tfidf = tfidf[corpus]  # 得到每个文本的tfidf向量，稀疏矩阵\n",
    "# #     print(\"corpus_tfidf is:\")\n",
    "# #     print(corpus_tfidf)\n",
    "#     lda = models.LdaMulticore(corpus_tfidf, id2word=dic, workers = 3,num_topics=200)\n",
    "# #     lda_1 = \n",
    "#     corpus_lda = lda[corpus_tfidf]  # 每个文本对应的LDA向量，稀疏的，元素值是隶属与对应序数类的权重\n",
    "#     print(\"lda is:\")\n",
    "#     print(lda)\n",
    "# #     print(\"lda\")\n",
    "\n",
    "#     num = 0\n",
    "#     for doc in corpus_lda:\n",
    "#         wstr = \"\"\n",
    "#         for i in range(len(doc)):\n",
    "#             item = doc[i]\n",
    "#             wstr += str(item[0]) + \",\" + str(item[1])[0:7] + \"/\"\n",
    "#         fo2.write(id_list[num] + \"\\t\" + wstr[0:-1] + \"\\n\")\n",
    "#         num += 1\n",
    "#     fr.close()\n",
    "#     fo2.close()\n",
    "#     print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5496547b222a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpost_cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpost_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;31m# lda_cluster = False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-159032062222>\u001b[0m in \u001b[0;36mpost_cut\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mkey_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# get keywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;31m#             print(\"key_list is \",key_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mustr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\73722\\Anaconda3\\lib\\site-packages\\jieba\\analyse\\tfidf.py\u001b[0m in \u001b[0;36mextract_tags\u001b[0;34m(self, sentence, topK, withWeight, allowPOS, withFlag)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mallowPOS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflag\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowPOS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\73722\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(self, sentence, cut_all, HMM)\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                 \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcut_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\73722\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36m__cut_DAG\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    250\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                             \u001b[0mrecognized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinalseg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecognized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                                 \u001b[1;32myield\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\73722\\Anaconda3\\lib\\site-packages\\jieba\\finalseg\\__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m__cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mForce_Split_Words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\73722\\Anaconda3\\lib\\site-packages\\jieba\\finalseg\\__init__.py\u001b[0m in \u001b[0;36m__cut\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m__cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0memit_P\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BMES'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_P\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans_P\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memit_P\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnexti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[1;31m# print pos_list, sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\73722\\Anaconda3\\lib\\site-packages\\jieba\\finalseg\\__init__.py\u001b[0m in \u001b[0;36mviterbi\u001b[0;34m(obs, states, start_p, trans_p, emit_p)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mem_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memit_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMIN_FLOAT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             (prob, state) = max(\n\u001b[0;32m---> 49\u001b[0;31m                 [(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in PrevStatus[y]])\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mV\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mnewpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\73722\\Anaconda3\\lib\\site-packages\\jieba\\finalseg\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mem_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memit_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMIN_FLOAT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             (prob, state) = max(\n\u001b[0;32m---> 49\u001b[0;31m                 [(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in PrevStatus[y]])\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mV\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mnewpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "url = \"data\"\n",
    "time1 = time.time()\n",
    "post_cut(url)\n",
    "post_tfidf(url)\n",
    "# lda_cluster = False\n",
    "# post_lda(url, False)\n",
    "\n",
    "# print(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, matutils\n",
    "count = 0\n",
    "fr = open(url + \"/post_key.txt\",encoding='utf8')\n",
    "fo2 = open(url + \"/post_vec_lda.txt\", \"w\",encoding='utf8')\n",
    "id_list = []\n",
    "data_list = []\n",
    "\n",
    "for line in fr.readlines():\n",
    "    term = line.strip().split(\"\\t\")\n",
    "    if len(term) == 2:\n",
    "        count += 1\n",
    "        id_list.append(term[0])\n",
    "        word = term[1].strip().split()\n",
    "        data_list.append(word)\n",
    "print(\"lda\")\n",
    "dic = corpora.Dictionary(data_list)  # 构造词典\n",
    "corpus = [dic.doc2bow(text) for text in data_list]  # 每个text 对应的稀疏向量\n",
    "#     print(\"corpus is:\")\n",
    "#     print(corpus)\n",
    "tfidf = models.TfidfModel(corpus)  # 统计tfidf\n",
    "#     print(\"lda\")\n",
    "corpus_tfidf = tfidf[corpus]  # 得到每个文本的tfidf向量，稀疏矩阵\n",
    "#     print(\"corpus_tfidf is:\")\n",
    "#     print(corpus_tfidf)\n",
    "lda = models.LdaMulticore(corpus_tfidf, id2word=dic, workers = 3,num_topics=200)\n",
    "#     lda_1 = \n",
    "corpus_lda = lda[corpus_tfidf]  # 每个文本对应的LDA向量，稀疏的，元素值是隶属与对应序数类的权重\n",
    "print(\"lda is:\")\n",
    "print(lda)\n",
    "#     print(\"lda\")\n",
    "\n",
    "num = 0\n",
    "for doc in corpus_lda:\n",
    "    wstr = \"\"\n",
    "    for i in range(len(doc)):\n",
    "        item = doc[i]\n",
    "        wstr += str(item[0]) + \",\" + str(item[1])[0:7] + \"/\"\n",
    "    fo2.write(id_list[num] + \"\\t\" + wstr[0:-1] + \"\\n\")\n",
    "    num += 1\n",
    "fr.close()\n",
    "fo2.close()\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = url = \"E:\\pythonNotebook\\data\"\n",
    "pd.set_option('max_colwidth', 10000)\n",
    "post_data = pd.read_table(url+ \"/post_data.txt\",encoding='utf8',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# post_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # cols = post_data.col\n",
    "# s = str(post_data[post_data.id=='109']['content '])\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models.L"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
